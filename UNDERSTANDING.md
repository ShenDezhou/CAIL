#2018CAIL
1、挑战赛任务：

任务一  罪名预测：根据刑事法律文书中的案情描述和事实部分，预测被告人被判的罪名；

任务二  法条推荐：根据刑事法律文书中的案情描述和事实部分，预测本案涉及的相关法条；

任务三  刑期预测：根据刑事法律文书中的案情描述和事实部分，预测被告人的刑期长短。

参赛者可选择一个或者多个任务参与挑战赛。同时，为了鼓励参赛者参与到更多的任务中，组委会将单独奖励参与更多任务的参赛者。

2、数据介绍：

本次挑战赛所使用的数据集是来自“中国裁判文书网”公开的刑事法律文书，其中每份数据由法律文书中的案情描述和事实部分组成，同时也包括每个案件所涉及的法条、被告人被判的罪名和刑期长短等要素。

数据集共包括268万刑法法律文书，共涉及183条罪名，202条法条，刑期长短包括0-25年、无期、死刑。

我们将先后发布CAIL2018-Small和CAIL2018-Large两组数据集。CAIL2018-Small包括19.6万份文书样例，直接在该网站发布，包括15万训练集，1.6万验证集和3万测试集。这部分数据可以自由下载，供参赛者前期训练和测试。比赛开始2-3周后（具体时间请关注比赛新闻），我们将通过网络下载向有资格的参赛队伍定向发布CAIL2018-Large数据集，包括150万文书样例。最后，剩余文书将作为第一阶段的测试数据CAIL2018-Large-test。

3、评价方法：

本次挑战赛使用的数据集均为来自中国裁判文书网上的刑事法律文书，标准答案是案件的判决结果。下面将对三项任务的评价方法分别进行说明：

任务一、任务二的评价方式：

任务一（罪名预测）、任务二（法条推荐）两项任务将采用分类任务中的微平均F1值（Micro-F1-measure）和宏平均F1值（Macro-F1-measure）作为评价指标。

任务三评价方式：

任务三（刑期预测）将根据预测出的刑期与案件标准刑期之间的差值距离作为评价指标。

三项任务总分的计算方式：

每个任务的满分均为100，则总分为：score_all = score_1 + scroe_2 + score_3

具体请参考github(https://github.com/thunlp/CAIL)。

4、基线系统：

竞赛组织方已提供一个开源的针对不同任务的基线系统（LibSVM（https://github.com/thunlp/CAIL2018/tree/master/baseline））

5、数据集论文：

本次比赛数据集论文CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction 已公布。数据下载地址是CAIL2018数据集下载

第一阶段（2018.05.15-2018.07.14） :  

开启报名，发放CAIL2018-Small数据，用于编写模型进行训练和测试。每周限提交3次，开放排行榜；

第一阶段开始3周之后（计划6月5日，可能根据参赛队伍情况调整，具体时间请关注官方网站公告栏），根据参赛者提交结果情况。对于高于任一任务预设基准算法成绩（已经在排行榜上显示svm_baseline，成绩为71.83、68.79、47.83）的队伍”，我们将通过网络下载定向发布CAIL2018-Large数据集，包括新增的150万份样例。发放结束后将于1周之内（计划6月12日，可能根据参赛队伍情况调整，具体时间请关注官方网站公告栏）利用全部测试数据CAIL2018-Large-test进行重新评测，刷新排行榜。第一阶段的最终成绩以各参赛队伍7月14日之前提交的最终比赛模型（或最后提交的模型）在全部测试数据CAIL2018-Large-test上的成绩为准。

第二阶段（2018.07.14-2018.08.14） :  

封闭评测，第一阶段结束时，所有参赛者提交最终比赛模型（或以最后提交的模型为准）。同时，主办方将收集中国裁判文书网在随后一个月内每天新增的裁判文书数据作为新的测试集，对各参赛者的模型进行封闭评测，得到最终成绩。

挑战赛的最终成绩计算方式：最终成绩 = 第一阶段的成绩 * 0.3 + 第二阶段的成绩 * 0.7

技术交流和颁奖活动（2018.09）

#2019CAIL
任务一（阅读理解）：裁判文书中包含了丰富的案件信息，比如时间、地点、人物关系等等，通过机器智能化地阅读理解裁判文书，可以更快速、便捷地辅助法官、律师以及普通大众获取所需信息。本任务是首次基于中文裁判文书的阅读理解比赛，属于篇章片段抽取型阅读理解比赛（Span-Extraction Machine Reading Comprehension）。为了增加问题的多样性，参考英文阅读理解比赛SQuAD和CoQA，本比赛增加了拒答以及是否类（YES/NO）问题。
任务二（要素识别）：本任务的主要目的是为了将案件描述中重要事实描述自动抽取出来，并根据领域专家设计的案情要素体系进行分类。案情要素抽取的结果可以用于案情摘要、可解释性的类案推送以及相关知识推荐等司法领域的实际业务需求中。具体地，给定司法文书中的相关段落，系统需针对文书中每个句子进行判断，识别其中的关键案情要素。本任务共涉及三个领域，包括婚姻家庭、劳动争议、借款合同等领域。
任务三（相似案例匹配）：本任务是针对多篇法律文书进行相似度的计算和判断。具体来说，对于每份文书我们提供文书的标题和事实描述，选手需要从两篇候选集文书中找到与询问文书更为相似的一篇文书。为了减小选手的工作量，我们相似案例匹配的数据只涉及民间借贷、知识产权纠纷和海商海事这三类文书。

##1.阅读理解
1.任务介绍

裁判文书中包含了丰富的案件信息，比如时间、地点、人物关系等等，通过机器智能化地阅读理解裁判文书，可以更快速、便捷地辅助法官、律师以及普通大众获取所需信息。

本任务是首次基于中文裁判文书的阅读理解比赛，属于篇章片段抽取型阅读理解比赛（Span-Extraction Machine Reading Comprehension）。

为了增加问题的多样性，参考英文阅读理解比赛SQuAD和CoQA，本比赛增加了拒答以及是否类（YES/NO）问题。

2.数据介绍

本任务技术评测使用的数据集由科大讯飞提供，数据主要来源于裁判文书网公开的裁判文书，其中包含刑事和民事一审裁判文书。

训练集约包含4万个问题，开发集和测试集各约5000个问题。

对于开发集和测试集，每个问题包含3个人工标注参考答案。

鉴于民事和刑事裁判文书在事实描述部分差异性较大，相应的问题类型也不尽相同，为了能同时兼顾这两种裁判文书，从而覆盖大多数裁判文书，本次比赛会设置民事和刑事两类测试集。

数据集详细介绍可以参见https://github.com/china-ai-law-challenge/CAIL2019/tree/master/阅读理解

3.评价方式

本任务采用与CoQA比赛一致的宏平均（macro-average F1）进行评估。

对于每个问题，需要与N个标准回答计算得到N个F1，并取最大值作为其F1值。

然而在评估人类表现（Human Performance）的时候，每个标准回答需要与N-1个其它标准回答计算F1值。

为了更公平地对比指标，需要把N个标准回答按照N-1一组的方式分成N组，最终每个问题的F1值为这N组F1的平均值。

整个数据集的F1值为所有数据F1的平均值。

更详细的评价方法详见https://github.com/china-ai-law-challenge/CAIL2019/tree/master/阅读理解

4.基线系统

我们将提供了两组基线模型，包括BiDAF模型和基于BERT的基线模型。详见阅读理解基线系统

##2.案情抽取
1.任务介绍

本任务的主要目的是为了将案件描述中重要事实描述自动抽取出来，并根据领域专家设计的案情要素体系进行分类。

案情要素抽取的结果可以用于案情摘要、可解释性的类案推送以及相关知识推荐等司法领域的实际业务需求中。

具体地，给定司法文书中的相关段落，系统需针对文书中每个句子进行判断，识别其中的关键案情要素。

本任务共涉及三个领域，包括婚姻家庭、劳动争议、借款合同等领域。

2.数据介绍

本任务所使用的数据集主要来自于“中国裁判文书网”公开的法律文书，每条训练数据由一份法律文书的案情描述片段构成，其中每个句子都被标记了对应的类别标签（需要特别注意的是，每个句子对应的类别标签个数不定），例如：

[{"sentence": "郭某诉李某离婚纠纷案2005年郭某与李某相识并确立恋爱关系，2006年3月26日生育一女，2006年4月18日郭某在北京市通州区太玉园小区购买房屋一套。",
"labels": ["MF-001", " MF-002"]},
{"sentence": " 2007年2月28日郭某与李某在吉林省某县民政局办理结婚登记手续，结婚证备注一栏写明：“补办登记，开始同居日期：2005-05-01",
"labels": []}]。

本次比赛所采用的数据均由具有法律背景的专业人士进行标注，但由于个人理解的差异，可能会存在极少部分的偏差或漏标等问题，还望参赛选手能够理解。

3.评价方式

本任务将采用多标签分类任务中的微平均F1值（Micro-F1-measure）和宏平均F1值（Macro-F1-measure）作为评价指标。

更详细的评价方法可以参考2018年法研杯任务二（法条推荐）的评价指标：https://github.com/china-ai-law-challenge/CAIL2019/tree/master/要素识别。

4.基线系统

我们提供了基于SVM实现的基线模型，参赛选手可以通过（后续补充）中获取相关源代码及注意事项。

##3.相似类案
1.任务介绍

本任务是针对多篇法律文书进行相似度的计算和判断。

具体来说，对于每份文书我们提供文书的标题和事实描述，选手需要从两篇候选集文书中找到与询问文书更为相似的一篇文书。

为了减小选手的工作量，我们相似案例匹配的数据只涉及民间借贷这类文书。

2.数据介绍

本任务所使用的数据集是来自“中国裁判文书网”公开的法律文书,其中每份数据由三篇法律文书组成。

对于每篇法律文书，我们仅提供事实描述。

对于每份数据，我们用(d, d1, d2)来代表该组数据，其中d,d1,d2均对应某一篇文书。

对于训练数据，我们保证，我们的文书数据d与d1的相似度是大于d与d2的相似度的，即sim(d, d1) > sim(d, d2)。

我们的数据总共涉及三万组文书三元对，所有的文书三元组对都一定属于民间借贷。

3.评价方式

对于测试数据，每组测试数据的形式与训练数据一致为(d1, d2, d3)但是此时我们不再保证sim(d, d1) > sim(d, d2)。

选手需要预测最终的结果是sim(d, d1) > sim(d, d2)还是sim(d, d1) < sim(d, d2)。

如果预测正确，那么该测试点选手可以得到1分，否则是0分。

最后选手的成绩为在所有数据上的得分平均值。

更详细的评价方法可以参考https://github.com/china-ai-law-challenge/CAIL2019/tree/master/相似案例匹配。

4.基线系统

我们提供了两组基线模型，包括基于tf-idf的基线模型和基于bert的语言基线模型，你可以在相似案例匹配基线系统中找到它们。

#2020CAIL
##阅读理解
任务介绍

CAIL2019上我们提出了中文司法阅读理解任务，今年我们将提出升级版，不仅文书种类由民事、刑事扩展为民事、刑事、行政，问题类型也由单步预测扩展为多步推理，难度有所升级。

具体而言，对于给定问题，只通过单句文本很难得出正确回答，选手需要结合多句话通过推理得出答案。

我们允许选手使用去年的阅读理解数据集（CJRC）作为辅助数据集，同时允许选手使用任何外部资料作为知识来帮助模型，但是我们要求选手在预测过程中不能够进行联网的操作。

更多的详细信息以及下文提到的资源都可以参考https://github.com/china-ai-law-challenge/CAIL2020/tree/master/ydlj。

数据介绍

本任务技术评测训练集包括两部分，一部分为去年的CJRC训练集，一部分为重新标注的约3000个问答对，其中民事、刑事、行政各1000个问答对，均为需要多步推理的问题类型。验证集和测试集各约3000个问答对，同样均为需要多步推理的问题类型。

评价方式

本任务采用F1进行评估。

对于每个问题，需要结合案情描述内容，给出回答，回答为Span（内容的一个片段）、YES/NO、Unknown中的一种，并且给出答案依据，即所有参与推理的句子编号。评价包括两部分：1）Answer-F1，即预测答案会与标准答案作比较，计算F1；2）SupFact-F1，即预测句子编号序列会与标准句子编号序列作比较，计算F1。最终为这两部分F1的联合F1宏平均。

基线系统

我们将提供一组基线模型，即BERT的阅读理解模型。

##摘要
任务介绍

裁判文书是人民法院公开审判活动、裁判理由、裁判依据和裁判结果的重要载体。司法摘要则是对裁判文书的内容进行压缩、归纳和总结，反映案件审理过程中的裁判过程、事实、理由和判决依据等。裁判文书司法摘要对我国法治建设具有现实意义与必要性。

具体来说，我们会提供裁判文书的原文，选手的任务是输出对应的司法摘要文本。

我们允许选手使用任何外部资料作为知识来帮助模型，但是我们要求选手在预测过程中不能够进行联网的操作，

更多的详细信息以及下文提到的资源都可以参考https://github.com/china-ai-law-challenge/CAIL2020/tree/master/sfzy。

数据介绍

本任务技术评测使用的训练集、验证集、测试集来自由北京司法大数据院提供并标注的法院裁判文书，包含大约10000篇裁判文书以及对应的司法摘要，其中所涉及到的裁判文书均为民事一审判决书。

评价方式

本任务采用ROUGE(Recall-Oriented Understudy for Gisting Evaluation)评价评价。

ROUGE指标将自动生成的摘要与参考摘要进行比较, 其中ROUGE-1衡量unigram匹配情况，ROUGE-2衡量bigram匹配，ROUGE-L记录最长的公共子序列。

基线系统

我们将提供BERT摘要基准模型。

##考试
任务介绍

司法考试作为我国最难的考试，也是法律工作者生涯中极其重要的考试，其难度以及淘汰率都是有目共睹的。因此，本任务是针对司法考试的问答提出的任务。

具体来说，我们会提供司法考试的题面以及选项，选手的任务是输出最后的题目的答案。

我们允许选手使用任何外部资料作为知识来帮助模型，但是我们要求选手在预测过程中不能够进行联网的操作，

更多的详细信息以及下文提到的资源都可以参考https://github.com/china-ai-law-challenge/CAIL2020/tree/master/sfks。

数据介绍

本任务技术评测使用的训练集、验证集1来自于论文JEC-QA: A Legal-Domain Question Answering Dataset，包含大约26,000道司法考试的题目，均为选择题。数据的更多细节可以参考该篇论文。验证集2来自于由专业人士新出的司法考试题，最终测试集会使用2020年的司法考试作为测试集。

评价方式

本任务采用准确率进行评估。

对于每个问题，需要给定这个题的答案，即A,B,C,D中的哪几项是正确的。当且仅当给定的答案与标准答案完全一致时才认为是正确的答案。

我们会额外给出选手模型在不同类型题目上的分数。

基线系统

我们将提供两组基线模型，包括随机模型和BERT的分类模型。

##辩论
任务介绍

在法院的庭审过程中，裁判文书起着记录辩、诉双方观点证据的重要作用。而诉方与辩方由于立场观点的不同，或是对于事实陈述上的不一致，这样便形成了庭审过程中双方的争议焦点，其是整场庭审的关键，也是裁判文书中的精髓。因此，本任务旨在抽取出裁判文书中辩方诉方之间的逻辑交互论点对，即争议焦点。

具体来说，我们会提供裁判文书中辩诉双方的陈述文本，选手的任务是输出存在逻辑交互关系的论点对。

我们允许选手使用任何外部资料作为知识来帮助模型，但是我们要求选手在预测过程中不能够进行联网的操作，

更多的详细信息以及下文提到的资源都可以参考https://github.com/china-ai-law-challenge/CAIL2020/tree/master/lbwj。

数据介绍

本任务技术评测使用的训练集、验证集、测试集来自由北京司法大数据院提供并标注的法院裁判文书，包含大约1000篇裁判文书以及4000对互动论点对。对于每个互动论点对中的诉方论点，我们都设置了五句辩方论点作为候选项，其中包含一句正确的辩方论点以及四句错误的辩方论点。

评价方式

本任务采用准确率进行评估。

对于每个诉方论点，需要给定候选辩方论点中的答案。

基线系统

我们将提供两组基线模型，包括随机模型和基于神经网络的分类模型。