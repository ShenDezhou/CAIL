Two steps to run baseline system

First step:Data process to get the data for trainning
./preprocess.sh input_data_dir output_data_dir # input_data_dir should contain the train.json and dev.json, output_data_dir is used to save generated data

Second step:Start to train the model
python run_cail.py \
    --name train_v1 \
    --bert_model bert_dir \ #bert weight direct,we used RoBERTa model weight
    --data_dir output_data_dir \ #the same as the output_data_dir in First step
    --batch_size 2 \
    --eval_batch_size 32 \
    --lr 1e-5 \
    --gradient_accumulation_steps 1 \
    --epochs 10 \

Baseline results(evaluate metrics like HotpotQA)
                          ans_em     ans_f1    sup_em   sup_f1   joint_em   joint_f1
Baseline                  0.6046	0.6915      0.3444	0.5496    0.2602	0.3906
Baseline  +  CAIL19 data  0.6658	0.7545      0.3929	0.5814    0.3163	0.4543


Output file format(you can see ouput_example.json)ï¼š
Output file is a json file dict.
{
"answer": {
"id":output_answer
}
 "sp": {
 "id":[[tile,sentid1],[tile,sentid2]]
 }

}

Evaluation

python evaluate.py predict.json test.json

#log
1. 20200624 BERT batch_size=2, gradient_accumulation_steps=1
{'em': 0.8, 'f1': 0.8, 'prec': 0.8, 'recall': 0.8, 'sp_em': 0.4, 'sp_f1': 0.72, 'sp_prec': 0.7333333333333333, 'sp_recall': 0.7333333333333333, 'joint_em': 0.4, 'joint_f1': 0.72, 'joint_prec': 0.7333333333333333, 'joint_recall': 0.7333333333333333}
2. 20200624 BERT batch_size=8, gradient_accumulation_steps=4

